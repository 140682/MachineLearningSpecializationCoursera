{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    id_test = []\n",
    "    y_train = []\n",
    "    with open('products_sentiment_train.tsv') as f:\n",
    "        for line in f:\n",
    "            parts = line.rsplit('\\t', 1)\n",
    "\n",
    "            X_train.append(parts[0].strip())\n",
    "            y_train.append(parts[1].strip())\n",
    "    \n",
    "    with open('products_sentiment_test.tsv') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            parts = line.split('\\t', 1)\n",
    "            id_test.append(parts[0].strip())\n",
    "            X_test.append(parts[1].strip())                    \n",
    "\n",
    "    return X_train, y_train, id_test, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(predictor, data_train, y, id_test, data_test, cv_score=None):\n",
    "    predictor.fit(data_train, y)\n",
    "    prediction = predictor.predict(data_test)\n",
    "    #print predictor\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    filepath_prediction = 'data/prediction-%s-data.csv' % timestamp\n",
    "    filepath_description = 'data/prediction-%s-estimator.txt' % timestamp\n",
    "\n",
    "    # Create a dataframe with predictions and write it to CSV file   \n",
    "    predictions_df = pd.DataFrame(data=prediction, columns=['y'])\n",
    "    predictions_df.to_csv(filepath_prediction, sep=',', index_label='Id')\n",
    "\n",
    "    # Write a short description of the classifier that was used\n",
    "    f = open(filepath_description, 'w')\n",
    "    f.write(str(predictor))\n",
    "    score = '\\nCross-validation score %.8f' % cv_score    \n",
    "    f.write(score)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(X_train, X_test):\n",
    "    vocab = set()\n",
    "    cv = CountVectorizer()\n",
    "    tokenizer = cv.build_tokenizer()\n",
    "    for line in X_train:\n",
    "        vocab.update(tokenizer(line))\n",
    "    for line in X_test:\n",
    "        vocab.update(tokenizer(line))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_1(vocab):\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(vocabulary=vocab)),\n",
    "        ('logreg', LogisticRegression()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.6, 0.8, 1.0),\n",
    "        'vect__min_df': (0, 1, 2, 5),\n",
    "        'vect__stop_words': ('english', None),\n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams        \n",
    "        'logreg__C': (0.0001, 0.01, 1),\n",
    "        'logreg__penalty': ('l2', 'l1'),        \n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_2(vocab):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(vocabulary=vocab)),\n",
    "        ('logreg', LogisticRegression()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'tfidf__max_df': (0.6, 0.8, 1.0),\n",
    "        'tfidf__min_df': (0, 1, 2, 5),\n",
    "        'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),        \n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_3(vocab):\n",
    "    # gives 0.7895 on cross-validation and 0.835 on Kaggle. done in 108.629s\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "        'vect__ngram_range': ((1, 3), (1, 2)),  # unigrams or bigrams\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        'clf__alpha': (0.00001, 0.000001),\n",
    "        'clf__penalty': ('l2', 'elasticnet'),\n",
    "        'clf__n_iter': (10, 50, 80),\n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pipeline_and_params_4(vocab):\n",
    "    nltk_sw = nltk_stop_words.words('english')\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "    parameters = {\n",
    "        'vect__max_df': (0.4, 0.5, 0.6),\n",
    "        #'vect__vocabulary': (None),\n",
    "        #'vect__stop_words': ('english', nltk_sw, None),\n",
    "        #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "        'vect__ngram_range': ((1,5), (1, 4), (2, 3), (2,4), (3, 5)),  # unigrams or bigrams\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', ),\n",
    "        'clf__alpha': (0.000001,),\n",
    "        'clf__penalty': ('elasticnet',),\n",
    "        'clf__n_iter': (10,),\n",
    "    }\n",
    "    return pipeline, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_grid_search(pipeline, parameters, X_train, y_train):\n",
    "    grid_search = GridSearchCV(pipeline, parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "    \n",
    "    print(\"Best score: %.4f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_experiment(X_train, y_train, id_test, X_test, vocab, get_pipeline_and_params):      \n",
    "    pipeline, parameters = get_pipeline_and_params(vocab)\n",
    "    gs = do_grid_search(pipeline, parameters, X_train, y_train)\n",
    "    predict(gs.best_estimator_, X_train, y_train, id_test, X_test, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, id_test, X_test = read_data() \n",
    "vocab = build_vocabulary(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 87.184s\n",
      "Best score: 0.7740\n",
      "Best parameters set:\n",
      "\tlogreg__C: 1\n",
      "\tlogreg__penalty: 'l2'\n",
      "\tvect__max_df: 0.6\n",
      "\tvect__min_df: 0\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, vocab, get_pipeline_and_params_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 16.710s\n",
      "Best score: 0.7560\n",
      "Best parameters set:\n",
      "\ttfidf__max_df: 0.6\n",
      "\ttfidf__min_df: 0\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, vocab, get_pipeline_and_params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 108.629s\n",
      "Best score: 0.7895\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'l2'\n",
      "\ttfidf__norm: 'l1'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, vocab, get_pipeline_and_params_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 18.123s\n",
      "Best score: 0.7910\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\ttfidf__norm: 'l1'\n",
      "\tvect__max_df: 0.6\n",
      "\tvect__ngram_range: (1, 4)\n"
     ]
    }
   ],
   "source": [
    "do_experiment(X_train, y_train, id_test, X_test, vocab, get_pipeline_and_params_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
